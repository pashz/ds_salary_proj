{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML cheatsheets\n",
    "### ML comparisons\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling & Centering (Normalization)\n",
    "\n",
    " - StandardScaler, MinMax, Robust\n",
    " - Robust may handle outliers better\n",
    " - It doesnt effect Tree based model\n",
    " - But it never harms the model, so can be used in all cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance Dataset\n",
    " - SMOTE\n",
    " - Oversampling\n",
    " - Precision / Recall Curve is better for Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold Cross validation methods\n",
    " - Train, Test \n",
    "    - Isolate Test\n",
    "    - CV on training set\n",
    " - Kfold\n",
    "    - Loop over 5 folds\n",
    "    - Manual\n",
    "    - only use this method if the feature engineering you do is very complex\n",
    " - cross_val_score, cross_validate\n",
    "    - Pipeline\n",
    "    - Get a feel for how your model to perform\n",
    "    - How do new features affect performance\n",
    "    - Using different algos\n",
    "    - Simple Hyperparameter (max_depth= [5, 10]) check for ranges\n",
    " - GridSearch\n",
    "    - Pipeline\n",
    "    - Complex Hyperparameter tuning with all combination of hyperparameters\n",
    "    - Automatically does cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Techniques\n",
    "\n",
    " - **Regression/Estimation** (*Predicting Continuous values*) technique is used for predicting a continuous value. For example, predicting things like the price of a house based on its characteristics, or to estimate the Co2 emission from a carâ€™s engine. \n",
    " - **Classification** (*Predicting the item class/category of a case*) technique is used for Predicting the class or category of a case, for example, if a cell is benign or malignant, or whether or not a customer will churn. \n",
    " - **Clustering** (*Finding the structure of data; summarization*) groups of similar cases, for example, can find similar patients, or can be used for customer segmentation in the banking field. \n",
    " - **Association** (*Associating frequent co-occuring items/events*) technique is used for finding items or events that often co-occur, for example, grocery items that are usually bought together by a particular customer. \n",
    " - **Anomaly detection** (*Discovering abnormal and unusual cases*) is used to discover abnormal and unusual cases, for example, it is used for credit card fraud detection. \n",
    " - **Sequence mining** (*Predicting next events;clickstream : Markoc Model, HMM*) is used for predicting the next event, for instance, the click-stream in websites. \n",
    " - **Dimension reduction** (*PCA*) is used to reduce the size of data. \n",
    " - **Recommendation systems**, (*Recommending items*) this associates people's preferences with others who have similar tastes, and recommends new items to them, such as books or movies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised\n",
    " - Regression\n",
    " - Classification\n",
    "\n",
    "### Unsupervised\n",
    " - **Dimension reduction** : Dimensionality reduction, and/or feature selection, play a large role in this by reducing redundant features to make the classification easier.\n",
    " - **Density estimation** : Density estimation is a very simple concept that is mostly used to explore the data to find some structure within it.\n",
    " - **Market basket analysis**: Market basket analysis is a modeling technique based upon the theory that if you buy a certain group of items, you're more likely to buy another group of items.\n",
    " - **Clustering**: Clustering is considered to be one of the most popular unsupervised machine learning techniques used for grouping data points, or objects that are somehow similar. Cluster analysis has many applications in different domains, whether it be a bank's desire to segment his customers based on certain characteristics, or helping an individual to organize in-group his, or her favorite types of music. Generally speaking though, clustering is used mostly for discovering structure, summarization, and anomaly detection.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/sup_unsup.jpg\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/Regression_Algos.jpg\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/Classification_Algos.jpg\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/train_test.jpg\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/KFold.jpg\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/errors.jpg\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Mean absolute error is the mean of the absolute value of the errors. This is the easiest of the metrics to understand, since it's just the average error. \n",
    " - Mean squared error is the mean of the squared error. It's more popular than mean absolute error because the focus is geared more towards large errors. This is due to the squared term exponentially increasing larger errors in comparison to smaller ones. \n",
    " - Root mean squared error is the square root of the mean squared error. This is one of the most popular of the evaluation metrics because root mean squared error is interpretable in the same units as the response vector or y units, making it easy to relate its information. \n",
    " - Relative absolute error, also known as residual sum of square, where y bar is a mean value of y, takes the total absolute error and normalizes it by dividing by the total absolute error of the simple predictor. \n",
    " - Relative squared error is very similar to relative absolute error but is widely adopted by the data science community, as it is used for calculating R squared. \n",
    " - R squared is not an error per se but is a popular metric for the accuracy of your model. It represents how close the data values are to the fitted regression line. The higher the R-squared, the better the model fits your data. \n",
    " \n",
    "##### Each of these metrics can be used for quantifying of your prediction. The choice of metric completely depends on the type of model, your data type, and domain of knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimating Multiple Linear Regression approaches:\n",
    " - Ordinary Least Squares\n",
    "    - Linear Algebra Operations\n",
    "    - Takes longer time for large dataset\n",
    " - An Optimization Algorithm\n",
    "   - Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Lifecycle\n",
    "\n",
    "#### Objective\n",
    " - Business Case\n",
    " - Metric\n",
    "\n",
    "#### Data\n",
    " - Data Collection\n",
    " - Cleaning\n",
    " - EDA\n",
    " - Feature Engineering\n",
    " \n",
    "#### ML\n",
    " - Model\n",
    " - Tuning\n",
    " - Evaluation\n",
    " - Tuning\n",
    " \n",
    "#### Reporting/Deployment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When use KNN?\n",
    " - Great for small Data.\n",
    "\n",
    " - If we have lots of features and data KNN dont perform very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM maximises distance between support vectors on each side to create the Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest - Wisdom of the crowd, Average of the guesses of many.\n",
    " - It look for random data and features eg 60% of the data, of sqr root Features eg 4 features of a total of 16.\n",
    " - if we have 100 trees, each will look at different snapshot of data, and then do agg for all.\n",
    " - Relatively fast\n",
    " - Requires less cleaning\n",
    " - hence used as a Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble method, looks at different snapshot of data and then create decision boundaries based on its perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trees Algorithm have Feature Importances\n",
    "##### Regression have coefficients\n",
    "\n",
    " - Remove less important features\n",
    " - Do Feature Engineering to important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision = TP / (TP + FP) Gives a measure of Predicted Positives (False Positives costs the model) \n",
    "eg Churn incentives given to Customers that might leave we need Precision\n",
    "\n",
    "Recall = TP / (TP + FN) Gives a measure of Actual Positives (False Negatives costs the model)\n",
    "eg Fraud Detection, Disease Prediction\n",
    "\n",
    "#### Regression metrics\n",
    " - Mean Squared Error\n",
    " - R2\n",
    "\n",
    "#### Classification metrics\n",
    " - Logloss\n",
    " - Accuracy\n",
    " - Precision\n",
    " - Recall\n",
    " - F1\n",
    " \n",
    "#### Adjust classification threshold to change ratio of prec:recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try every possible split for the Features, to get towards the pure node based on maximum depth. Also look for Gini/Entropy to check how much the data is mixed.\n",
    "\n",
    "Loop through all the features.\n",
    "\n",
    "Try different parameters to make it less mixed.\n",
    "\n",
    "Gini Impurity is the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset. \n",
    "https://victorzhou.com/blog/gini-impurity/\n",
    "\n",
    "Information Gain is calculated for a split by subtracting the weighted entropies of each branch from the original entropy. When training a Decision Tree using these metrics, the best split is chosen by maximizing Information Gain.\n",
    "https://victorzhou.com/blog/information-gain/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use One Hot Encoding for Categorical Variables. Decision Trees doesnt care about weightage, Label Encoding can also be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias - Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for Over-fitting or under-fitting:\n",
    " - Check for Train, Test Accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "print('Train score:', accuracy_score(y_train, y_pred_train))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('Test score:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we make this model better?\n",
    " - Use different ML algorithm\n",
    " - Feature Engineering\n",
    " - Hyperparameter Tuning\n",
    " - Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "objective -> \n",
    "\n",
    "data -> model -> reporting/deployment data collection = data engineer \n",
    "\n",
    "eda/feature engineering/model = data scientist \n",
    "\n",
    "deployment = machine learning engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leakage\n",
    " - leaking data into train/test set\n",
    " - in Vinnys example Customer ID's relationship with label causes it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Learning\n",
    " - The Algorithm gets really good in learning majority Class and not so well in minority class.\n",
    " - The Accuracy score as metric is no longer useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Oversampling\n",
    " - Take the Minority Class and randomly duplicate them\n",
    " - You can also Undersample or use a combination of both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have lots of Data then you can use Holdout Cross Validation (Millions of rows of Data). \n",
    "\n",
    "But if you have smaller Data then you can use K-Fold Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the K-Fold Score **varies** a lot then the model is **Unstable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/SVM_app.jpg\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
